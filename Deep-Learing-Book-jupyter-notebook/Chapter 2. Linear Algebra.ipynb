{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "# 2 Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p31-1:\n",
    "+ 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p31-2:\n",
    "+ 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 39**):\n",
    "+ 벡터는 수의 배열로 표현 되며 크기를 구할 수 있는데, 이 값을 Norm(놈)이라 부름\n",
    "+ 수식적으로 $L^{p}$ Norm은 다음과 같이 정의된다. $$||x||_p = \\left(\\sum_i { |x_i|^{p} } \\right)^{\\frac{1}{p}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ Norm은 원점으로부터의 거리로 해석할 수 있으며, 다음 3가지 공식을 만족 해야 한다.\n",
    "\n",
    "$$f(x) = 0 \\implies \\textbf{x} = \\textbf{0} $$\n",
    "$$f(x + y) \\leq f(x) + f(y) \\;\\;(삼각 부등식)$$\n",
    "$$\\forall\\alpha, f(\\alpha x) = |\\alpha|f(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 여기서 p=1 일 때의 Norm을 \"맨해탄 Norm\"이라 부르며, $L^{1}$ 으로 나타낼 수 있다.\n",
    "+ 여기서 p=2 일 때의 Norm을 \"유클리디언 Norm\"이라 부르며, $L^{2}$ 으로 나타낼 수 있다.\n",
    "+ 이 $L^{2}$기계학습에서 매우 자주 사용되며, 수식으로 나타낼 때 에도 $||x||_2$ 가 아니라 2를 생략하고 $||x||$ 로 표현한다.\n",
    "+ 수식적으로 간단히 $x^{\\top} x$로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 실제 기계학습에서 $L^{2}$를 사용 할 때는 결과에 제곱을 해서 사용한다.\n",
    "+ $L^{2}$는 직관적으로 제곱을 하여 사용하기 때문에 outlier(이상점)의 정도가 민감하게 바뀐다. 1을 기준으로 적다면 적게 바뀌고 많다면 많이 바뀐다.\n",
    "+ $L^{1}$은 머신러닝에서 에러를 구하는 방법 중 하나인 최소절대편차(Least Absolute Deviations, LAD)에 사용된다.\n",
    "+ $L^{2}$는 머신러닝에서 에러를 구하는 방법 중 하나인 최소제곱오차(Least Squares Error)에 사용된다.\n",
    "\n",
    "$$||x||_1 = \\sum_i { |x_i| }  $$\n",
    "$$||x||_1 = |x_1| + |x_2| + |x_3| + ... + |x_i|$$\n",
    "$$S = \\sum_{i=1}^n |{y_i - f(x_i) }|$$\n",
    "\n",
    "\n",
    "$$||x||_2 = \\left(\\sum_i { |x_i|^{2} } \\right)^{\\frac{1}{2}}  $$\n",
    "$$||x||_2 = \\sqrt{|x_1|^{2} + |x_2|^{2} + |x_3|^{2} + ... + |x_i|^{2}}$$\n",
    "$$S = \\sum_{i=1}^n \\left({y_i - f(x_i) }\\right)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2 (**page 40**):\n",
    "+ $L^{1}$은 outlier(이상치)의 변화에 '적당히' 영향을 미친다.\n",
    "+ 그렇기 때문에 0과 0이 아닌 요소의 차이가 매우 중요한 경우에 사용할 수 있다.\n",
    "+ 항상 0점으로 부터 $\\epsilon$ 만큼 증가 할 때 똑같이 $\\epsilon$ 만큼 증가한다. (선형성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ $L^{0}$은 열벡터에서 0이 아닌 요소의 수를 세는데 사용된다.\n",
    "+ 하지만 $L^{0}$은 올바른 표현이 아니다. 3번째 공식을 만족하지 않으므로 이 기능이 Norm이라 표현하는 것은 오류다.\n",
    "+ $L^{1}$는 0이 아닌 요소의 수를 대체하는데 사용된다. (이해 안됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 열벡터 내부에 가장 큰 요소를 뽑아 내는 것은 $L^{\\infty}$(max norm)이라 한다. \n",
    "\n",
    "$$||x||_\\infty = \\underset{i}{max} { |x_i| }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 행렬의 크기를 구하는 Norm을 Frobenius norm이라 한다.\n",
    "\n",
    "$$||A||_F = \\left(\\sum_{i,j} { A^{2}_{i,j} } \\right)^{\\frac{1}{2}}  $$\n",
    "\n",
    "+ $L^{2}$와 비슷함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 두 벡터의 내적의 계산은 Norm으로 표현 가능하며, x 벡터와 y 벡터가 $\\theta$ 각도를 이룰 때 다음과 같다.\n",
    "\n",
    "$$x^{\\top}y = ||x||_2||y||_2\\cos{\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Special Kinds of Matrices and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p6:\n",
    "+ 일부 유용한 특수 행렬과 특수 벡터가 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p7:\n",
    "+ 대각행렬 $D$\n",
    "+ $i \\neq j$인 경우 $D_{i,j} = 0$ 이다.\n",
    "+ 대표적으로 항등행렬 $I$(또는 $E$)가 있음\n",
    "+ 대각 행렬만 뽑아내 벡터로 만들었을 때 그 벡터를 $v$라고 한다면, 대각행렬의 순서가 $v$와 같은 배열을 $diag(v)$라고 한다.\n",
    "+ 이는 한 벡터에 각각의 다른 고유한 숫자를 곱할 때 유용하다.\n",
    "\n",
    "+ 행렬 $A$이 대각 행렬이고, 대각선 성분이 $v=[1,2,3]^{\\top}$일 경우, $A$는 $diag(v)$로 표현될 수 있다.\n",
    "+ 벡터 $x=[2,4,6]^{top}$과 $diag(v)$를 곱하면, 벡터 $v$와 벡터 $x$의 각 요소를 곱한 것과 같다.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "diag(v)x & = & [v_1x_1, v_2x_2, v_3x_3] \\\\\n",
    "& = & [1 \\times 2, 2 \\times 4, 3 \\times 6]\n",
    "\\end{eqnarray}\n",
    "\n",
    "+ 더욱 간단히 $v \\odot x$으로 표현할 수 있다.\n",
    "+ $diag(v)$의 역함수는 $diag(v)^{-1}$로 표현하며, $diag(v)^{-1}= diag([1/v_1, 1/v_2, ... , 1/v_n])$으로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 41**):\n",
    "+ 대각행렬 $D$가 꼭 정사각행렬일 필요는 없다.\n",
    "+ 정사각행렬이 아닌 $D$는 역행렬이 존재하지 않는다.\n",
    "+ 곱해지는 벡터 $x$의 일정 수치의 고차원 요소를 0으로 만들고자 할 때(스케일링) 사용한다. ($D$가 $i \\times j$ 행렬 일 때, $i$개 이상의 요소는 전부 0이 된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ 대칭행렬(Symmetric matrix)는 행렬에 트랜스포즈(Transpose : $A^{\\top}$) 연산을 수행 해도 원래의 $A$와 같은 행렬을 말한다.\n",
    "$$A = A^{\\top}$$\n",
    "+ 무조건 정사각행렬 이어야 하며, $A$의 모든 원소에 대해 $A_{i,j} = A_{j,i}$를 만족하는 행렬을 말한다.\n",
    "+ 일반적으로 어떠한 규칙에 따라 행렬을 만들 때 순서에 상관 없는 경우 대칭행렬이 만들어진다.\n",
    "+ 예를 들어 단순이 좌표평면 $(i,j)$의 거리를 나타내는 행렬을 만들 경우 $i$에서 $j$의 거리나, $j$에서 $i$의 거리는 같기 때문에 대칭행렬로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 단위 벡터(unit vector)는 단위 노름(unit norm)으로 정의된다.\n",
    "$$||x||_2 = 1  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ $x^{\\top}y = 0$인 경우, 두 벡터 $x$와 $y$는 서로 **직교** 한다고 말한다. \n",
    "+ 두 벡터가 직교 하면서 둘 다 단위 벡터라면, 이를 정규직교라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 직교 행렬은 열벡터들과 행벡터들이 서로 직교하는 행렬을 말한다.\n",
    "+ 따라서 다음의 식을 만족한다.\n",
    "\n",
    "$$A^{\\top}A = AA^{\\top} = I$$\n",
    "+ 즉,\n",
    "$$A^{-1}=A^{\\top}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 42**)\n",
    "+ 역행렬은 많은 계산량이 필요하나, 전치행렬은 계산량이 적게 소모되어, 이를 응용 가능\n",
    "+ There is no special term for a matrix whose rows or columns are orthogonal but not orthonormal. (무슨의미?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ 많은 수학적 object는 그것들을 부분으로 쪼개거나 일부 속성들을 표현함으로써 더 잘 이해할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 예를 들어 정수는 소인수 분해 하여 소수들의 곱으로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 행렬도 마찬가지로 행렬의 열벡터들이 표현 가능한 공간의 기저가 되는 부분들을 뽑아낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 이를 위한 분해 기법 중 하나를 고유값 분해(Eigen decomposition)라고 부르며, 여기에서 고유값 분해를 통해 **고유 벡터**과 **고유값**을 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p6:\n",
    "+ 정사각행렬 $A$의 고유 벡터를 $v$라 한다면, $A$와 $v$를 곱하면 단지 $v$의 스케일만 변경되게 된다.\n",
    "+ 서로 다른 행렬 $A$와 $B$를 곱하면 전혀 관계 없는 $C$가 나오지만, $A$의 고유 벡터를 곱하게 되면 관계 없는 $C$가 나오는게 아니라 단지 $\\lambda$만큼의 스케일 변화가 된 $v$가 나오게 된다.\n",
    "$$Av = \\lambda v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p7:\n",
    "+ 여기서 $\\lambda$는 고유값이라 한다.\n",
    "+ $v^{\\top}A = \\lambda v^{\\top}$ 과 같은 왼쪽 고유벡터를 구할수도 있다. (계속 열벡터를 중심으로 설명 했지만, 행벡터를 계산한다는 뜻)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p8:\n",
    "+ $s \\in \\mathbb{R}, s \\neq 0$를 만족하는 어떠한 상수 $s$를 곱해도 고유벡터는 $v$로, 변하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p9:\n",
    "+ 행렬 $A$에 대해 $n$개의 선형 독립적인 고유벡터들이 존재 할 때, 모든 고유벡터들을 연결하여 행렬 $V$를 만들 수 있다.\n",
    "+ 같은 방법으로 각각의 고유벡터들에 상응하는 고유값 $\\lambda$들을 연결하여 벡터를 만들 수 있고, 이를 $\\lambda$라고 하면, 대각행렬 $diag(\\lambda)$를 만들 수 있다.\n",
    "+ 이들을 이용한 고유값 분해는 다음과 같이 주어진다.\n",
    "\n",
    "$$A = V diag(\\lambda)V^{-1}$$\n",
    "\n",
    "+ 위와 같은 식을 유도 해 내기 위해서는 행렬식 개념을 알아야 하며, 꽤나 복잡한 과정(단순 산술 연산이 매우 많음)의 수식을 거쳐야한다. 하지만 이 책에서 고유값 분해의 과정 자체는 중요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p10:\n",
    "+ 고유값과 구유벡터로 행렬을 분해 하면, 원하는 방향으로 공간을 늘리는 것과 같은 응용 동작을 쉽게 설명할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p11:\n",
    "+ 모든 행렬들이 고유값 분해가 되지는 않는다.\n",
    "+ 만약 $A$가 대칭 행렬일 경우 다음과 같이 정리될 수 있다.\n",
    "\n",
    "$$ A = Q \\Lambda Q^{\\top}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 44**)\n",
    "+ 여기서 $Q$는 $A$의 고유벡터의 직교행렬이고, $\\Lambda$ 는 $diag(\\lambda)$이다.\n",
    "+ $Q$는 직교행렬 이므로 각 $i$번째 열벡터 성분마다 $\\lambda_i$만큼 늘린 것으로 볼 수 있다.\n",
    "![image1](img/image2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ $A$의 고유 벡터들은 중복이 될 수 있다. \n",
    "+ 이 경우는 2차 방정식의 끝 값이 $y=0$ 를 지날 때 해가 1개 인 것 처럼, 고유값 분해에서도 중복되는 고유벡터들이 나올 가능성이 존재 한다는 뜻 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 고유값중 하나가 0인 경우 영공간이 존재하며, **'특이'**한 경우로 판단된다.\n",
    "+ 예를 들어, 대칭행렬 $A$에 대해 고유값이 3개 나오며, 이 중 1개의 값이 0인 경우에는 행렬 $A$는 3개의 3차원 벡터로 이루어져있지만, 하나의 차원은 영공간이 되므로 2차원 만 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 고유값이 모두 양수인 경우 **positive definite**라 한다.\n",
    "+ 고유값이 모두 양수이거나 모두 0인 경우 **positive semidefinite**라 한다.\n",
    "+ 고유값이 모두 음수인 경우 **negative definite**라 한다.\n",
    "+ 고유값이 모두 음수이거나 모두 0인 경우 **negative semidefinite**라 한다.\n",
    "+ [대칭행렬의 고유값 부호](https://datascienceschool.net/view-notebook/7fd58178d9e64be682058db7e024d8b5/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singlar Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ SVD (Singular Value Decomposition) 또한 행렬을 분해하는 방법 중 하나 이다.\n",
    "+ SVD가 더 많은 경우에도 동작한다. 특히 정사각행렬이 아니어도 분해 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 45**)\n",
    "+ 고유값 분해는 다음의 2개의 행렬로 표현될 수 있다.\n",
    "\n",
    "$$A = V diag(\\lambda)V^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ 특이값 분해는 3개의 행렬로 표현된다.\n",
    "\n",
    "$$A = UDV^{\\top}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ $A$는 m x n 행렬이라고 가정 하면, $U$는 m x m, $D$는 m x n, $V$는 n x n으로 정의된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 행렬 $U$와 $V$는 직교 유니터리 행렬로 정의된다.\n",
    "+ 유니터리 행렬이란 켤레 전치가 역행렬과 같은 복소수 행렬이다.\n",
    "+ 행렬 $D$는 대각행렬 이다. 또한 반드시 정사각행렬 일 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 행렬 $D$의 대각 성분은 $A$의 특이값으로 구성된다.\n",
    "+ 행렬 $U$의 열은 좌측 특이벡터로 이루어져있다.\n",
    "+ 행렬 $V$의 열은 우측 특이벡터로 이루어져있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p6:\n",
    "+ $A$의 좌측 특이벡터는 $AA^{\\top}$의 고유벡터와 같다.\n",
    "+ $A$의 우측 특이벡터는 $A^{\\top}A$의 고유벡터와 같다.\n",
    "+ $A$의 0이 아닌 특이값은 $AA^{\\top}$의 고유값의 제곱근과 같다. $A^{\\top}A$와도 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p7:\n",
    "+ 특이값 분해에 대한 유용한 기능들은 다음 세션에 소개된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
